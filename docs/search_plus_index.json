{"./":{"url":"./","title":"前言","keywords":"","body":"编程字典 记录常用的命令操作。 前言 第一章 Linux 1.1 基础命令 1.2 SHELL 1.3 系统监控 1.4 Windows 第二章 语言 2.1 Python 2.2 Java 第三章 数据库 3.1 MySQL 3.2 Redis 第四章 工具 4.1 Git 4.2 Maven 4.3 IDEA 4.4 Nginx 第五章 大数据 5.1 Kafka 5.2 ES 5.3 HDFS 5.4 Hive 5.5 Flume 5.6 Zookeeper 5.7 Flink 第六章 教程 6.1 Flink 中文课程 结语 Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-05-27 13:10:50 "},"chapter1/":{"url":"chapter1/","title":"第一章 Linux","keywords":"","body":"第一章 Linux 1.1 基础命令 1.2 SHELL 1.3 FAQ 1.4 Windows Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-05-22 11:10:58 "},"chapter1/基础命令.html":{"url":"chapter1/基础命令.html","title":"1.1 基础命令","keywords":"","body":"1.1 基础命令 [TOC] 修改主机名 hostnamectl set-hostname test.cn 环境变量 永久生效 vim /etc/profile source /etc/profile 暂时生效 export PATH=$PATH:/usr/local/MATLAB/R2013a/bin 只对当前用户永久生效 vim ~/.bash_profile source ~/.bash_profile 压缩命令 unzip # 解压到当前目录 unzip *.zip # 解压到指定目录，不覆盖 unzip -n *.zip -d /tmp # 解压到指定目录，覆盖 unzip -o *.zip -d /tmp tar # 解压 tar -zxvf *.tar.gz # 压缩 tar -zcvf *.tar.gz # 查看 tar -ztvf *.tar.gz 安装fpm yum -y install ruby rubygems ruby-devel gem sources -a http://mirrors.aliyun.com/rubygems/ gem sources --remove https://rubygems.org/ gem install fpm 打包时出现报错Need executable 'rpmbuild' to convert dir to rpm {:level=>:error}。 安装rpm工具 yum install -y rpm-build 分区操作 lvm相关操作1 # 查看pv pvs pvscan pvdisplay # 查看vg vgs vgscan vgdisplay # 查看lvm分区 lvs lvscan lvdisplay 查看磁盘使用 df -h 查看分区使用 fdisk -l lvm分区及挂载 # 查看块设备信息 [root@localhost /]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 2T 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 512M 0 part /boot ├─sda3 8:3 0 200M 0 part /boot/efi └─sda4 8:4 0 2T 0 part ├─vg_cobbler-lv_root 253:0 0 512G 0 lvm / └─vg_cobbler-lv_swap 253:1 0 16G 0 lvm [SWAP] # 查看vgs的空余大小 [root@localhost /]# vgs VG #PV #LV #SN Attr VSize VFree vg_cobbler 1 3 0 wz--n- 硬盘格式化并挂载 # 查看块设备信息 [root@localhost /]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 19G 0 part ├─centos-root 253:0 0 17G 0 lvm / └─centos-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 20G 0 disk sdc 8:32 0 20G 0 disk sr0 11:0 1 1024M 0 rom # 可以看到sdb和sdc为未挂载的硬盘 # 格式化sdb并挂载到/data01 # 1. 创建/data01目录 [root@localhost /]# mkdir /data01 # 2. 快速格式化sdb [root@localhost /]# mkfs.ext4 /dev/sdb mke2fs 1.42.9 (28-Dec-2013) /dev/sdb is entire device, not just one partition! Proceed anyway? (y,n) y Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) Stride=0 blocks, Stripe width=0 blocks 1310720 inodes, 5242880 blocks 262144 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=2153775104 160 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000 Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): done Writing superblocks and filesystem accounting information: done # 3. 挂载到/data01 [root@localhost /]# mount /dev/sdb /data01 # 4. 添加开机自动挂载配置 [root@localhost /]# vim /etc/fstab /dev/sdb /data01 ext4 defaults 0 0 # 再次查看块设备信息 [root@localhost /]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot └─sda2 8:2 0 19G 0 part ├─centos-root 253:0 0 17G 0 lvm / └─centos-swap 253:1 0 2G 0 lvm [SWAP] sdb 8:16 0 20G 0 disk /data01 sdc 8:32 0 20G 0 disk sr0 11:0 1 1024M 0 rom # 查看文件系统 [root@localhost /]# df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/centos-root 17G 15G 2.4G 87% / /dev/sda1 1014M 234M 781M 23% /boot /dev/sdb 20G 45M 19G 1% /data01 分区调整 首先确认系统在安装时, 根和/home是否做的是lvm, 如果做了lvm, 那就可以在线进行根分区的扩容3: 1) 先用vgdisplay 查看系统是否有空间空间, 如果还有, 就扩容到根分区. 2) 如果系统没有空闲空间, 就只能从其他lvm分区上缩减出一定的空间, 然后扩容到根分区. 3) 你的/home分区是xfs格式的,缩容之前, 需要先关闭该分区上部署的应用, 把有用数据拷贝到别的分区上, 接着进行缩容, 缩容后重新挂载, 然后再把数据拷贝回来, 重启应用.最后再将/home分区缩容出来的空间在线扩容到根分区上. 使用lvresize调整分区大小后，务必使用xfs_growfs应用分区调整（用词不一定准确，待修改）。 # 压缩home lvm分区30G大小空间，调整后必须对home重新格式化才能使用，不然会报错 lvresize -L -30G /dev/mapper/centos-home # 增加root lvm分区30G大小空间 lvresize -L +30G /dev/mapper/centos-root # 或 lvextend -L +30G /dev/mapper/centos-root # 此时使用lvs命令可以看到lvm分区的大小实现了调整，但是df查看文件系统块的大小却没有发现变化 # 调整xfs格式的root lvm分区文件块，xfs_growfs只支持分区增加容量 xfs_growfs /dev/mapper/centos-root # 如果xfs使用resize2f调整分区文件块会报错 resize2fs /dev/mapper/centos-root > resize2fs 1.42.9 (28-Dec-2013) > resize2fs: Bad magic number in super-block while trying to open /dev/mapper/centos-root > Couldn't find valid filesystem superblock. 删除lvm分区 lvremove /dev/centos/home 强制取消挂载 # 查看目录占用 fuser -m /home # 解除占用 fuser -kvm /home # 强制取消挂载 umount -l /home 非LVM分区转化为LVM分区 https://blog.csdn.net/it_is_a_world/article/details/101081754 多个硬盘扩展为一个LVM # 使用整个磁盘创建VG,设置PE大小为64M [root@localhost /]# vgcreate vg_data /dev/vdb /dev/vdc -s 64M # 查看PV [root@localhost /]# pvs # 查看VG [root@localhost /]# vgs # 查看VG详情 [root@localhost /]# vgdisplay vg_data # 使用vg_data创建LV,大小为20T [root@localhost /]# lvcreate -L 20T -n lv_data01 vg_data [root@localhost /]# vgs [root@localhost /]# lvs [root@localhost /]# lvdisplay # 格式化lv_data01 [root@localhost /]# mkfs.ext4 /dev/vg_data/lv_data01 # 创建data01 [root@localhost /]# mkdir /data01 # 挂载lv_data01到/data01 [root@localhost /]# mount /dev/vg_data/lv_data01 /data01 # 添加开机自动挂载配置 [root@localhost /]# vim /etc/fstab > /dev/vg_cobbler/lv_data01 /data01 ext4 defaults 0 0 # 再次查看块设备信息 [root@localhost /]# lsblk # 查看文件系统 [root@localhost /]# df -h su 与 sudo2 su su [-lm] [-c 命令] [username] 参数： -: 单纯使用-如su -，代表使用login-shell的变量文件读取方式来登录系统；不加用户名，则代表切换为root的身份。 -l: 与-类似，但后面需要加欲切换的用户账号。也是login-shell的方式。 -m: -m与-p是一样的，表示使用目前的环境设置，而不读取新用户的配置文件。 -c: 仅进行一次命令，所以-c后面可以加上命令。 sudo 仅需要自己的密码。 sudo [-b] [-u 新用户密码] 参数： -b: 将后续的命令让系统自行执行，而不与目前的shell产生影响。 -u: 后面接欲切换的用户，若无则默认切换root。 可执行sudo的用户与文件/etc/sudoers有关，编辑该文件需用visudo命令。 Login与non-login shell2 login shell: 取得bash时需要完整的登录流程。比如，要由tty1~tty6登录，需要输入用户的账号与密码，此时取得的bash就称为“login shell”。 non-login shell: 取得bash接口的方法不需要重复登录的举动。比如，以X Window登录Linux后，再以X的图形界面启动终端机，此时那个终端接口并没有需要再次输入账号与密码，那个bash的环境就称为non-login shell。在原本的bash环境下再次执行bash这个命令，同样也没有输入账号密码，那第二个bash（子进程）也是non-login shell。 端口 查看端口占用 lsof -i lsof -i:22 netstat -tunlp | grep 8080 netstat –anp | grep 8080 开放端口 # firewall # 开放tcp 端口 firewall-cmd --zone=public --add-port=80/tcp --permanent # 开放udp端口 firewall-cmd --zone=public --add-port=49999/udp --permanent # 刷新加载端口配置 firewall-cmd --reload # 查看开放端口 iptables -L -n # iptables iptables -t filter -I INPUT -p tcp -m tcp --dport 8080 -j ACCEPT 关闭端口 iptables -D INPUT -p tcp --dport 80 -j ACCEPT RPM # 导出目前安装的rpm rpm -qa > list.txt RPM假死 rm -f /var/lib/rpm/__db.00*　#删除rpm数据文件 rpm --rebuilddb　#重新rpm数据文件 查看网卡配置 # 看网关 netstat -rn # 看netmask，dns ifconfig -a ip addr 查看网络流量 sar -n DEV 1 4 查看JDK安装 rpm -qa | grep jdk Yum配置 查看源 yum repolist all 配置阿里源 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 配置光盘Yum源 # 挂载光盘 mkdir /media/cdrom mount /dev/cdrom /media/cdrom cd /etc/yum.repos.d # 备份原始源 mkdir backup mv CentOS* backup tar zcvf repos.backup.tar.gz backup mv backup/CentOS-Media.repo . # 修改源地址 vim CentOS-Media.repo > file:///media/cdrom/ > enable=1 # 清除包缓存 yum clean all # 生成缓存 yum makecache LN Linux链接分两种，一种被称为硬链接（Hard Link），另一种被称为符号链接（Symbolic Link）。默认情况下，ln命令产生硬链接。 硬链接相当于对文件进行了备份，防止误删。 软链接相当于Windows的快捷方式。 touch f1 # 创建f1的一个硬链接文件f2 ln f1 f2 # 创建f1的一个软链接文件f3 ln -s f1 f3 时区 # 查看时区 date -R # 修改时区为上海 cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime # 设置时区 timedatectl set-timezone Asia/Shanghai # 设置时间 timedatectl set-time \"YYYY-MM-DD HH:MM:SS\" # 将硬件时钟调整为与本地时钟一致 timedatectl set-local-rtc 1 hwclock --systohc --localtime SSH linux生成密钥 ssh-keygen -t rsa 组操作 # 强行设置某个用户所在组 usermod -g 用户组 用户名 # 把某个用户改为 group(s) usermod -G 用户组 用户名 # 把用户添加进入某个组(s） usermod -a -G 用户组 用户名 # 将用户从组中删除 gpasswd -d 用户名 用户组 Crontab crontab [-u username] [-l|-e|-r] # -u 只有 root 才能进行这个任务 # -l 查看工作内容 crontab -l # -e 编辑工作内容 crontab -e # -r 删除所有的工作内容 crontab -r # 启动服务 systemctl start crond.service # 关闭服务 systemctl stop crond.service # 重启服务 systemctl restart crond.service # 重新载入配置 systemctl reload crond.service # 查看状态 systemctl status crond.service # 查看日志 tail -1000f /var/log/cron 将crontab文件放在/etc/cron.d下，然后执行systemctl restart crond.service重启crontab服务。 crontab 不执行的原因 未重启crond服务/未重载（reload）crond配置 文件换行不是unix crontab文件末尾没有空行 系统版本 cat /etc/redhat-release uname -a # centos 统计文件/目录 # 1、 统计当前文件夹下文件的个数 ls -l |grep \"^-\"|wc -l # 2、 统计当前文件夹下目录的个数 ls -l |grep \"^d\"|wc -l # 3、统计当前文件夹下文件的个数，包括子文件夹里的 ls -lR|grep \"^-\"|wc -l # 4、统计文件夹下目录的个数，包括子文件夹里的 ls -lR|grep \"^d\"|wc -l grep \"^-\" # 这里将长列表输出信息过滤一部分，只保留一般文件，如果只保留目录就是 ^d wc -l Jfrog 的Artifactory 命令的使用 预先设置 jfrog rt config --user= --password= --url=https://xxx.xx.xx/artifactory --interactive=false api与url 如果不预先设置，需要在命令后面添加--url=和--apikey= 目录的下载 jfrog.exe rt dl --url=“https://xxx.xxx.xx/artifactory” --threads=1 --split-count=0 目录上传 jfrog rt u --flat=false # 上传后，在服务器上的路径将是 + # 如果想上传目录层次比较少，可以先在本地进入需要上传的目录下，在执行jfrog命令 目录删除 jfrog rt del --quiet=true 查看MD5 md5sum 文件名 SYSLOG Mar 20 12:28:48 TAC systemd系统服务4 历史上，Linux 的启动一直采用init进程。 下面的命令用来启动服务。 sudo /etc/init.d/apache2 start # 或者 service apache2 start 这种方法有两个缺点。 一是启动时间长。init进程是串行启动，只有前一个进程启动完，才会启动下一个进程。 二是启动脚本复杂。init进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。 Systemd 就是为了解决这些问题而诞生的。它的设计目标是，为系统的启动和管理提供一套完整的解决方案。 根据 Linux 惯例，字母d是守护进程（daemon）的缩写。 Systemd 这个名字的含义，就是它要守护整个系统。 使用了 Systemd，就不需要再用init了。Systemd 取代了initd，成为系统的第一个进程（PID 等于 1），其他进程都是它的子进程。 # 查看 Systemd 的版本 systemctl --version Systemd 的优点是功能强大，使用方便，缺点是体系庞大，非常复杂。事实上，现在还有很多人反对使用 Systemd，理由就是它过于复杂，与操作系统的其他部分强耦合，违反\"keep simple, keep stupid\"的Unix 哲学。 （上图为 Systemd 架构图） init5 init是Linux系统操作中不可缺少的程序之一。　　 所谓的init进程，它是一个由内核启动的用户级进程。　　 内核自行启动（已经被载入内存，开始运行，并已初始化所有的设备驱动程序和数据结构等）之后，就通过启动一个用户级程序init的方式，完成引导进程。 所以,init始终是第一个进程（其进程编号始终为1）。 内核会在过去曾使用过init的几个地方查找它，它的正确位置（对Linux系统来说）是/sbin/init。如果内核找不到init，它就会试着运行/bin/sh，如果运行失败，系统的启动也会失败。 init一共分为7个级别，这7个级别的所代表的含义如下** 0：停机或者关机（千万不能将initdefault设置为0） 1：单用户模式，只root用户进行维护 2：多用户模式，不能使用NFS(Net File System) 3：完全多用户模式（标准的运行级别） 4：安全模式 5：图形化（即图形界面） 6：重启（千万不要把initdefault设置为6） CP 参数详解 -f ：为强制 (force) 的意思，若有重复或其它疑问时，不会询问使用者，而强制复制； -i ：若目的檔(destination)已经存在时，在覆盖时会先询问是否真的动作！ -r ：递归持续复制，用于目录的复制行为； cp命令，其实是cp -i命令，也就是不管我们怎么输入 cp -rf，其实执行的是cp -i -rf , 也无怪乎总是提问是否覆盖了。 覆盖与不覆盖 其实还有一种更简单的方法可以解决这个问题，就是在cp前加一个反斜杠，如： \\cp name.text /usr/home/ rsync删除文件 针对上面的问题，我们可以通过以下方法清空该目录： 先创建一个空目录 mkdir /tmp/empty/ 清空目标目录 rsync --delete-before -avH --progress --stats /tmp/empty/ /var/spool/postfix/maildrop rsync --delete -rlptD /tmp/empty/ /var/spool/postfix/maildrop/ 选项说明： -delete-before 接收者在传输之前进行删除操作 –progress 在传输时显示传输过程 -a 归档模式，表示以递归方式传输文件，并保持所有文件属性 -H 保持硬连接的文件 -v 详细输出模式 –stats 给出某些文件的传输状态 不过在使用上面的命令进行清理时，存在一个问题，清空后，目标目录的权限会和源目录的权限一样。如：/tmp/empty是root：root，而maildrop之前是postfix：postdrop ，执行之后也会maildrop目录的权限也会变成root：root 。由于-a权限是-rlptogD几个参数的集合，所以可以将og（owner:group）两个参数去掉。清空时自动保持之前的目录权限，如下： rsync --delete -rlptD /tmp/empty/ /var/spool/postfix/maildrop/ rsync与rm 删除速度比较 为什么rsync这么快呢？ rm删除内容时，将目录的每一个条目逐个删除(unlink)，需要循环重复操作很多次；rsync删除内容时，建立好新的空目录，替换掉老目录，基本没开销 开机启动 查看 systemctl get-default 命令界面 systemctl set-default multi-user.target 图形界面 systemctl set-default graphical.target systemd系统服务4 历史上，Linux 的启动一直采用init进程。 下面的命令用来启动服务。 sudo /etc/init.d/apache2 start # 或者 service apache2 start 这种方法有两个缺点。 一是启动时间长。init进程是串行启动，只有前一个进程启动完，才会启动下一个进程。 二是启动脚本复杂。init进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。 Systemd 就是为了解决这些问题而诞生的。它的设计目标是，为系统的启动和管理提供一套完整的解决方案。 根据 Linux 惯例，字母d是守护进程（daemon）的缩写。 Systemd 这个名字的含义，就是它要守护整个系统。 使用了 Systemd，就不需要再用init了。Systemd 取代了initd，成为系统的第一个进程（PID 等于 1），其他进程都是它的子进程。 # 查看 Systemd 的版本 systemctl --version Systemd 的优点是功能强大，使用方便，缺点是体系庞大，非常复杂。事实上，现在还有很多人反对使用 Systemd，理由就是它过于复杂，与操作系统的其他部分强耦合，违反\"keep simple, keep stupid\"的Unix 哲学。 （上图为 Systemd 架构图） init5 init是Linux系统操作中不可缺少的程序之一。　　 所谓的init进程，它是一个由内核启动的用户级进程。　　 内核自行启动（已经被载入内存，开始运行，并已初始化所有的设备驱动程序和数据结构等）之后，就通过启动一个用户级程序init的方式，完成引导进程。 所以,init始终是第一个进程（其进程编号始终为1）。 内核会在过去曾使用过init的几个地方查找它，它的正确位置（对Linux系统来说）是/sbin/init。如果内核找不到init，它就会试着运行/bin/sh，如果运行失败，系统的启动也会失败。 init一共分为7个级别，这7个级别的所代表的含义如下** 0：停机或者关机（千万不能将initdefault设置为0） 1：单用户模式，只root用户进行维护 2：多用户模式，不能使用NFS(Net File System) 3：完全多用户模式（标准的运行级别） 4：安全模式 5：图形化（即图形界面） 6：重启（千万不要把initdefault设置为6） CP 参数详解 -f ：为强制 (force) 的意思，若有重复或其它疑问时，不会询问使用者，而强制复制； -i ：若目的檔(destination)已经存在时，在覆盖时会先询问是否真的动作！ -r ：递归持续复制，用于目录的复制行为； cp命令，其实是cp -i命令，也就是不管我们怎么输入 cp -rf，其实执行的是cp -i -rf , 也无怪乎总是提问是否覆盖了。 覆盖与不覆盖 其实还有一种更简单的方法可以解决这个问题，就是在cp前加一个反斜杠，如： \\cp name.text /usr/home/ Curl curl -i -X '://:/?' -d '' -i: 显示头信息 被 标记的部件： VERB 适当的HTTP 方法 或 谓词 : GET、 POST、 PUT、 HEAD 或者DELETE。 PROTOCOL http或者 https HOST 主机名，或者用 localhost 代表本地机器上的节点。 PORT 端口号 PATH API 的终端路径 QUERY_STRING 任意可选的查询字符串参数 (例如 ?pretty 将格式化地输出 JSON 返回值，使其更容易阅读) BODY 一个 JSON 格式的请求体 (如果请求需要的话) 1. centos7下修改分区大小（LVM） ↩ 2. 鸟哥的Linux私房菜·基础学习篇（第三版），P428-433 ↩ 3. Linux下对lvm逻辑卷分区大小的调整（针对xfs和ext4不同文件系统） ↩ 4. Systemd 入门教程：命令篇 ↩ 5. linux 下的init 0，1，2，3，4，5，6知识介绍 ↩ Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-05-27 13:04:35 "},"chapter1/SHELL.html":{"url":"chapter1/SHELL.html","title":"1.2 SHELL","keywords":"","body":"1.2 SHELL [TOC] 判断是否为空 if [ -n \"$APP_ID\" ]; then echo \"is not empty\" fi if [ -z \"$APP_ID\" ]; then echo \"is empty\" fi if []; then elif []; then else fi 获取参数 #!/bin/bash echo $1 $2 # 获取当前进程id echo $! # 当前脚本的文件名 echo $0 数组循环 for type in ${TYPES[@]}; do TYPES_STR=\"$TYPES_STR $type\" done # 获取数组元素 echo \"数组的元素为: ${my_array[*]}\" echo \"数组的元素为: ${my_array[@]}\" # 获取数组长度 echo \"数组元素个数为: ${#my_array[*]}\" echo \"数组元素个数为: ${#my_array[@]}\" 基本循环 for ((i = 0; i 文件判断 # 文件不存在 if [ ! -f \"$file\" ]; then touch \"$file\" fi # 文件夹不存在 if [ ! -d \"$folder\"]; then mkdir \"$folder\" fi 数字大小判断 if [ $num1 -gt $num2 ] ; then echo \"$num1 > $num2\" else echo \"$num1 获取函数返回值 #!/bin/bash funWithReturn(){ echo \"这个函数会对输入的两个数字进行相加运算...\" echo \"输入第一个数字: \" read aNum echo \"输入第二个数字: \" read anotherNum echo \"两个数字分别为 $aNum 和 $anotherNum !\" return $(($aNum+$anotherNum)) } funWithReturn echo \"输入的两个数字之和为 $? !\" 字符串匹配 ==比较 使用bash检查字符串是否以某些字符开头可以使用==比较 [[ $str == h* ]] 示例 str=\"hello\" if [[ $str == h* ]]; then echo 'yes' fi 有两个地方需要注意： h*不需要使用引号括起来，使用引号括起来是直接做相等比较 比较语句使用双中括号括起来，而不是使用单中括号 =~正则比较 如果使用Bash的正则 str=\"hello\" if [[ \"$str\" =~ ^he.* ]]; then echo \"yes\" fi 使用正则匹配字符串的开头字符需要注意： he*：不要使用he*，这里的*号表示e字符0到多个，即h，以及heeee都是测试通过的 he.*：这里只允许包含he的字符串通过测试 ^he.*：这个表示是以he开头的字符串通过检测 命令找不到（command not found） export PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin source /etc/profile # 或者 /bin/vi /etc/profile 文件内容删除（sed） FILE=\"/usr/local/test.path\" CONTENT=\"/usr/local/test.txt\" # 将\"/\"替换为\"\\/\" CONTENT_REG=${CONTENT//\\//\\\\/} # 适用sed删除 sed -i ${CONTENT_REG}/d ${FILE} 替换JAVA_HOME PROFILE=/etc/profile # 配置环境变量 # 不包含已有的JAVA_HOME JAVA_HOME=/usr/java/jdk1.8.0_212-amd64 if [ $(grep -c \"export JAVA_HOME=\" $PROFILE) -eq 0 ]; then echo \"export JAVA_HOME=${JAVA_HOME}\" >>$PROFILE echo \"export JRE_HOME=\\${JAVA_HOME}/jre\" >>$PROFILE echo \"export CLASSPATH=.:\\${JAVA_HOME}/lib:\\${JRE_HOME}/lib\" >>$PROFILE echo \"export PATH=\\${PATH}:\\${JAVA_HOME}/bin\" >>$PROFILE else JAVA_HOME_REG=${JAVA_HOME//\\//\\\\/} sed -i \"s/^export JAVA_HOME.*\\$/export JAVA_HOME=$JAVA_HOME_REG/g\" $PROFILE fi $ 变量含义 linux中shell变量$#,$@,$0,$1,$2的含义解释: $$ # Shell本身的PID（ProcessID） $! # Shell最后运行的后台Process的PID $? # 最后运行的命令的结束代码（返回值） $- # 使用Set命令设定的Flag一览 $* # 所有参数列表。如\"$*\"用「\"」括起来的情况、以\"$1 $2 … $n\"的形式输出所有参数。 $@ # 所有参数列表。如\"$@\"用「\"」括起来的情况、以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。 $# # 添加到Shell的参数个数 $0 # Shell本身的文件名 $1～$n # 添加到Shell的各参数值 获取目录下的文件夹 ls -al $1 | awk '/^d/ {print $NF}' 获取目录下的文件 ls -al $1 | awk '/^-/ {print $NF}' Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-05-22 14:05:56 "},"chapter1/FAQ.html":{"url":"chapter1/FAQ.html","title":"1.3 系统监控","keywords":"","body":"1.3 系统监控 [TOC] service异常 Redirecting to /bin/systemctl restart mysqld.service Failed to restart mysqld.service: Activation of org.freedesktop.systemd1 timed out See system logs and 'systemctl status mysqld.service' for details. 造成原因是内存不足导致org.freedesktop.logind和org.freedesktop.systemd模块奔溃。 临时解决方案： systemctl daemon-reexec 再启动登录服务 systemctl start systemd-logind 系统监控 磁盘占用 iotop -oP 内存 free -h # 查看进程占用 cat /proc/【进程id】/status VmPeak: 11008236 kB VmSize: 11008140 kB VmLck: 0 kB VmPin: 0 kB VmHWM: 5126716 kB VmRSS: 5126716 kB VmData: 9433616 kB VmStk: 132 kB VmExe: 22168 kB VmLib: 3200 kB VmPTE: 15012 kB VmSwap: 0 kB sar系统监控 sar [options] [-A] [-o file] t [n] 其中： t为采样间隔，n为采样次数，默认值是1； -o file表示将命令结果以二进制格式存放在文件中，file 是文件名。 options 为命令行选项，sar命令常用选项如下： -A：所有报告的总和 -u：输出[CPU](http://lovesoo.org/tag/cpu)使用情况的统计信息 -v：输出inode、文件和其他内核表的统计信息 -d：输出每一个块设备的活动信息 -r：输出[内存](http://lovesoo.org/tag/内存)和交换空间的统计信息 -b：显示[I/O](http://lovesoo.org/tag/io)和传送速率的统计信息 -a：文件读写情况 -c：输出进程统计信息，每秒创建的进程数 -R：输出内存页面的统计信息 -y：终端设备活动情况 -w：输出系统交换活动信息 查看网速 sar -n DEV 1 100 Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-05-27 13:10:26 "},"chapter1/Windows.html":{"url":"chapter1/Windows.html","title":"1.4 Windows","keywords":"","body":"Windows 目录树（tree） # 文件夹 tree # 包含文件 tree /F # 适用ASCII字符 tree /A 查看MD5 # 查看MD5值： certutil -hashfile 文件名 MD5 # 查看 SHA1 certutil -hashfile 文件名 SHA1 # 查看SHA256 certutil -hashfile 文件名 SHA256 # 支持的算法有：MD2 MD4 MD5 SHA1 SHA256 SHA384 SHA512 Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-05-08 10:47:09 "},"chapter2/":{"url":"chapter2/","title":"第二章 语言","keywords":"","body":"第二章 语言 2.1 Python 2.2 Java Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-14 10:08:04 "},"chapter2/Python.html":{"url":"chapter2/Python.html","title":"2.1 Python","keywords":"","body":"2.1 Python [TOC] 离线安装库 # 1. 查看安装的包 pip list # 2. 生成依赖列表 pip freeze >requirements.txt # 3. 下载依赖文件的包到本地 pip download -d .\\packages -r .\\requirements.txt --trusted-host mirrors.aliyun.com # 4. 离线安装 pip install --no-index --find-links .\\packages -r .\\requirements.txt pip install numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl # 5. 安装 tar.gz tar -zxvf xxx.tar.gz cd xxx python setup.py install str bytes互转 # bytes object b = b\"example\" # str object s = \"example\" # str to bytes bytes(s, encoding = \"utf8\") # bytes to str str(b, encoding = \"utf-8\") # an alternative method # str to bytes str.encode(s) # bytes to str bytes.decode(b) Conda 强制卸载 conda remove --force name 离线安装 conda install --use-local pytorch-1.0.0-py2.7_cuda9.0.176_cudnn7.4.1_1.tar.bz2 Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-14 10:08:04 "},"chapter2/Java.html":{"url":"chapter2/Java.html","title":"2.2 Java","keywords":"","body":"2.2 Java [TOC] 基础 获取文件最后的修改时间 import java.io.File; import java.util.Date; public class Main { public static void main(String[] args) { File file = new File(\"Main.java\"); Long lastModified = file.lastModified(); Date date = new Date(lastModified); System.out.println(date); } } JAR无法读取配置文件 public static String readFileByStream(String fileName){ try { StringBuilder stringBuffer = new StringBuilder(); InputStream stream = FilesUtils.class.getClassLoader().getResourceAsStream(fileName); if (stream == null) { return null; } BufferedReader br = new BufferedReader(new InputStreamReader(stream, StandardCharsets.UTF_8)); String line; while ((line = br.readLine()) != null) { stringBuffer.append(line); } return stringBuffer.toString(); } catch (Exception e) { e.printStackTrace(); } return null; } 读取JSON文件中的unicode 使用JSONObject转化一遍。 JAVA参数 java -X -Xmixed 混合模式执行 (默认) -Xint 仅解释模式执行 -Xbootclasspath: 设置搜索路径以引导类和资源 -Xbootclasspath/a: 附加在引导类路径末尾 -Xbootclasspath/p: 置于引导类路径之前 -Xdiag 显示附加诊断消息 -Xnoclassgc 禁用类垃圾收集 -Xincgc 启用增量垃圾收集 -Xloggc: 将 GC 状态记录在文件中 (带时间戳) -Xbatch 禁用后台编译 -Xms 设置初始 Java 堆大小，默认为物理内存的1/64 -Xmx 设置最大 Java 堆大小，默认为物理内存的1/4 -Xss 设置 Java 线程堆栈大小 -Xprof 输出 cpu 配置文件数据 -Xfuture 启用最严格的检查, 预期将来的默认值 -Xrs 减少 Java/VM 对操作系统信号的使用 (请参阅文档) -Xcheck:jni 对 JNI 函数执行其他检查 -Xshare:off 不尝试使用共享类数据 -Xshare:auto 在可能的情况下使用共享类数据 (默认) -Xshare:on 要求使用共享类数据, 否则将失败。 -XshowSettings 显示所有设置并继续 -XshowSettings:all 显示所有设置并继续 -XshowSettings:vm 显示所有与 vm 相关的设置并继续 -XshowSettings:properties 显示所有属性设置并继续 -XshowSettings:locale 显示所有与区域设置相关的设置并继续 -X 选项是非标准选项, 如有更改, 恕不另行通知。 -Xmn 新生代大小，通过这个值也可以得到老生代的大小：-Xmx减去-Xmn -Xss 设置每个线程可使用的内存大小，即栈的大小。在相同物理内存下，减小这个值能生成更多的线程，当然操作系统对一个进程内的线程数还是有限制的，不能无限生成。线程栈的大小是个双刃剑，如果设置过小，可能会出现栈溢出，特别是在该线程内有递归、大的循环时出现溢出的可能性更大，如果该值设置过大，就有影响到创建栈的数量，如果是多线程的应用，就会出现内存溢出的错误。1 示例 -Xmx3550m -Xms3550m -Xmn2g -Xss128k Spring Boot 打包 为方便程序的安装与运行，将JAR与运行脚本打包为RPM包。 开启SSL2 生成证书 keytool -genkey -keyalg RSA -keysize 2048 -validity 3650 -keystore application.keystore -storepass PASS@2019 -storetype PKCS12 添加配置到配置文件application.properties server.port=443 server.http.port=80 ######## SSL ######## server.ssl.key-store=classpath:application.keystore server.ssl.key-store-password=PASS@2019 server.ssl.key-store-type=PKCS12 在启动类*Application.java配置连接器，同时启用HTTP和HTTPS // 兼容HTTP 和 HTTPS @Bean public ServletWebServerFactory servletContainer(){ TomcatServletWebServerFactory tomcat = new TomcatServletWebServerFactory(); tomcat.addAdditionalTomcatConnectors(createHTTPConnector()); return tomcat; } // HTTP自动跳转到HTTPS @Bean public TomcatServletWebServerFactory servletContainer() { TomcatServletWebServerFactory tomcat = new TomcatServletWebServerFactory() { @Override protected void postProcessContext(Context context) { SecurityConstraint securityConstraint = new SecurityConstraint(); securityConstraint.setUserConstraint(\"CONFIDENTIAL\"); SecurityCollection collection = new SecurityCollection(); collection.addPattern(\"/*\"); securityConstraint.addCollection(collection); context.addConstraint(securityConstraint); } }; tomcat.addAdditionalTomcatConnectors(httpConnector()); return tomcat; } private Connector createHTTPConnector() { Connector connector = new Connector(\"org.apache.coyote.http11.Http11NioProtocol\"); connector.setScheme(\"http\"); connector.setSecure(false); connector.setPort(httpPort); connector.setRedirectPort(httpsPort); return connector; } 启用Actuator org.springframework.boot spring-boot-starter-actuator management.endpoints.web.exposure.include=env,beans,health,autoconfig management.server.port=8110 management.server.servlet.context-path=/ management.server.ssl.enabled=false management.endpoint.health.show-details=always 查看状态 GET: http://{host}:{port}/actuator { \"_links\": { \"self\": { \"href\": \"http://localhost:8110/actuator\", \"templated\": false }, \"beans\": { \"href\": \"http://localhost:8110/actuator/beans\", \"templated\": false }, \"health-component-instance\": { \"href\": \"http://localhost:8110/actuator/health/{component}/{instance}\", \"templated\": true }, \"health-component\": { \"href\": \"http://localhost:8110/actuator/health/{component}\", \"templated\": true }, \"health\": { \"href\": \"http://localhost:8110/actuator/health\", \"templated\": false }, \"env\": { \"href\": \"http://localhost:8110/actuator/env\", \"templated\": false }, \"env-toMatch\": { \"href\": \"http://localhost:8110/actuator/env/{toMatch}\", \"templated\": true } } } 创建非Web项目 引入 org.springframework.boot spring-boot-starter 禁用Banner public static void main(String[] args) { SpringApplication springApplication = new SpringApplication(DynamiccalculateApplication.class); springApplication.setBannerMode(Banner.Mode.LOG); springApplication.run(args); } 实现 public interface CommandLineRunner { /** * Callback used to run the bean. * @param args incoming main method arguments * @throws Exception on error */ void run(String... args) throws Exception; } 运行参数 -Dspring.profiles.active=dev Logback(Slf4j)配置文件 在Spring Boot中默认使用Logback作为日志插件，Logback的默认配置文件为logback-spring.xml，文件内容示例如下。 logback ERROR --> ${log.pattern} %d{HH:mm:ss.SSS} %contextName [%thread] %-5level %logger{36} - %msg%n--> ${log.path}/${app.name}.%d{yyyy-MM-dd}.log ${log.pattern} %d{HH:mm:ss.SSS} %contextName [%thread] %-5level %logger{36} - %msg%n--> ${log.path}/${app.name}.%d{yyyy-MM-dd}.%i.log 30 128MB 1024MB ${log.pattern} %d{HH:mm:ss.SSS} %contextName [%thread] %-5level %logger{36} - %msg%n--> 异常 The server sockets created using the LocalRMIServerSocketFactory 日志错误 AM sun.rmi.transport.tcp.TCPTransport$AcceptLoop executeAcceptLoop WARNING: RMI TCP Accept-0: accept loop for ServerSocket[addr=0.0.0.0/0.0.0.0,localport=32658]s java.io.IOException: The server sockets created using the LocalRMIServerSocketFactory only aconnections from clients running on the host where the RMI remote objects have been exported. at sun.management.jmxremote.LocalRMIServerSocketFactory$1.accept(LocalRMIServerSockety.java:114) at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java: at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:372) at java.lang.Thread.run(Thread.java:745) 解决方法 1) fix etc hosts or 2) disable local host checks. Disabling local host checking can be done in two ways: a) system-wide: uncomment the line # com.sun.management.jmxremote.local.only=false in jre/lib/management/management.properties b) process based: pass -Dcom.sun.management.jmxremote.local.only=false on the java command line (attachee side) 1. JVM优化之 -Xss -Xms -Xmx -Xmn 参数设置 ↩ 2. springboot 2.X 配置SSL证书 ↩ Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-23 10:05:47 "},"chapter3/":{"url":"chapter3/","title":"第三章 数据库","keywords":"","body":"第三章 数据库 3.1 MySQL 3.2 Redis Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-14 10:31:13 "},"chapter3/MySQL.html":{"url":"chapter3/MySQL.html","title":"3.1 MySQL","keywords":"","body":"3.1 MySQL [TOC] 安装MySQL 卸载mariadb rpm -qa | grep mariadb rpm -e --nodeps mariadb-libs 在线安装 wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm sudo yum -y install mysql-server rpm安装 rpm -ivh mysql-community-common rpm -ivh mysql-community-lib rpm -ivh mysql-community-client rpm -ivh mysql-community-server service mysqld start 查看安装默认密码 cat /var/log/mysqld.log |grep password 修改密码 mysql > set global validate_password_policy=0; mysql > set global validate_password_length=1; mysql > ALTER USER USER() IDENTIFIED BY '123456'; mysql > use mysql; mysql > update user set password=password('123456') where user='root'; mysql > select host,user, password from user; mysql > GRANT ALL PRIVILEGES ON *.* TO root@\"%\" IDENTIFIED BY \"root\"; mysql > flush privileges; mysql > exit; service mysqld restart windows mysql 5.7 修改密码 mysql > use mysql; mysql > update user set authentication_string=password('新密码') where user='root' and Host='localhost'; mysql > flush privileges; mysql > exit; 允许远程 mysql > use mysql; mysql > GRANT ALL PRIVILEGES ON *.* TO root@\"%\" IDENTIFIED BY \"新密码\"; mysql > flush privileges; mysql > exit; service mysqld restart 开放端口 firewall-cmd --zone=public --add-port=3306/tcp --permanent firewall-cmd --reload iptables -L -n 查看连接占用 show processlist; show full processlist; show status; ON DUPLICATE KEY UPDATE 判断记录是否存在,不存在则插入存在则更新的场景. 举个例子，字段a被定义为UNIQUE，并且原数据库表table中已存在记录(2,2,9)和(3,2,1)，如果插入记录的a值与原有记录重复，则更新原有记录，否则插入新行： INSERT INTO TABLE (a,b,c) VALUES (1,2,3), (2,5,7), (3,3,6), (4,8,2) ON DUPLICATE KEY UPDATE b=VALUES(b); 以上SQL语句的执行，发现(2,5,7)中的a与原有记录(2,2,9)发生唯一值冲突，则执行ON DUPLICATE KEY UPDATE，将原有记录(2,2,9)更新成(2,5,9)，将(3,2,1)更新成(3,3,1)，插入新记录(1,2,3)和(4,8,2) 注意：ON DUPLICATE KEY UPDATE只是MySQL的特有语法，并不是SQL标准语法！ 删除字段为NULL DELETE FROM table_name WHERE type is NULL; 查看连接 show full processlist; 查看状态 show status; Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-05-13 14:50:58 "},"chapter3/Redis.html":{"url":"chapter3/Redis.html","title":"3.2 Redis","keywords":"","body":"3.2 Redis [TOC] 安装Redis # 下载 wget http://download.redis.io/releases/redis-5.0.5.tar.gz # 解压 tar xzf redis-5.0.5.tar.gz # 编译 cd redis-5.0.5 make # 将编译文件放于/usr/local/redis下 mkdir /usr/local/redis cp src/redis-cli /usr/local/redis cp src/redis-server /usr/local/redis cp redis.conf /usr/local/redis 允许远程 编辑配置文件vim /usr/local/redis/redis.conf，找到相应行修改： # bind 127.0.0.1 protected-mode no # 取消守护启动，用于开机自启 daemonize no 自启配置 添加服务脚本vim /etc/systemd/system/redis.service： [Unit] Description=Redis Server Manager After=syslog.target After=network.target [Service] # Type=simple PIDFile=/var/run/redis_6379.pid ExecStart=/usr/local/redis/redis-server /usr/local/redis/redis.conf ExecStop=/usr/local/redis/redis-cli shutdown Restart=always [Install] WantedBy=multi-user.target 添加服务，开机启动： systemctl daemon-reload systemctl start redis.service systemctl enable redis.service 添加redis-cli软链接： ln -s /usr/local/redis/redis-cli /usr/bin/redis-cli 开放端口 firewall-cmd --zone=public --add-port=6379/tcp --permanent firewall-cmd --reload iptables -L -n 基本操作 # 查看所有key keys * # 获取所有配置 config get * 错误 使用spring boot连接redis发现redis中数据无故无规律清空 a. 修改redis service文件vim /etc/systemd/system/redis.service b. 修改redis配置文件，增加logfile路径 c. 修改系统时区 然后就未出现当前问题，但是并未确定问题的原因。 Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-14 10:08:04 "},"chapter4/":{"url":"chapter4/","title":"第四章 工具","keywords":"","body":"第四章 工具 4.1 Git 4.2 Maven 4.3 IDEA 4.4 Nginx Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-05-27 13:04:35 "},"chapter4/Git.html":{"url":"chapter4/Git.html","title":"4.1 Git","keywords":"","body":"4.1 Git [TOC] 安装Git 下载 wget https://www.kernel.org/pub/software/scm/git/git-2.11.1.tar.gz 安装依赖 yum groupinstall \"Development Tools\" yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel perl-CPAN perl-devel perl-ExtUtils-Embed 解压安装 tar -zvxf git-2.11.1.tar.gz cd git-2.11.1 # 指向ssl安装位置 ./configure --with-openssl=/usr/local/openssl make install rpm -e git --nodeps ln -s /usr/local/bin/git /usr/bin/git git version 基本操作 # 更新远程仓库链接 git remote set-url origin xxxx # 推送指定分支到远端 git pull : # 拉取指定分支到本地 git push : # 取消commit但保留add git reset --soft HEAD^ # 取消commit和add git reset HEAD^ # 删除commit和add代码，恢复到上次的提交状态 git reset HEAD^ 子模块 # 添加子模块 git submodule add ${子模块仓库地址} ${子模块名称} # 拉取子模块 # 方法一：正常的使用git clone命令，然后再使用 git submodule init 和git submodule update来获取子模块 # 初始化子模块(只添加子模块版本的指针，文件夹是空的) git submodule init ${子模块名称} # 更新子模块 git submodule update ${子模块名称} # 方法二：在使用git clone命令时，加上–recurse-submodules或–recursive 这样的递归参数 git clone --recursive ${主仓库地址} .修改默认编辑器 以sublime为例，需将sublime_text.exe的路径添加到环境变量中。 git config --global core.editor sublime_text.exe 修改已commit的注释信息 git commit --amend 查看用户名和邮箱地址 git config user.name git config user.email 修改用户名和邮箱地址 git config --global user.name \"username\" git config --global user.email \"email\" 文件换行格式 # 提交时转换为LF，检出时不转换 git config --global core.autocrlf input # 拒绝提交包含混合换行符的文件 git config --global core.safecrlf true 查看所有的提交记录 git log 查看最新的commit git show 查看指定commit hashID的所有修改 git show commitId 查看某次commit中具体某个文件的修改 git show commitId fileName 回到与远程仓库一致处 git fetch --all git reset --hard origin/develop git pull 标签 # 新建一个标签 git tag # 可以指定标签信息 git tag -a v0.1 -m \"version 0.1 released\" # 查看所有标签 git tag # 推送标签 git push origin v1.5 # 删除标签 git tag -d v1.4-lw # 删除远程标签 git push origin :refs/tags/v1.4-lw # 检出标签 git checkout 2.0.0 修改 comment git commit --amend 清除本地的新增文件（未add） git clean -df 常见问题 fatal: write error: Broken pipe git config http.postBuffer 104857600 中文乱码处理 # 注释：该命令表示提交命令的时候使用utf-8编码集提交 git config --global i18n.commitencoding utf-8 # 注释：该命令表示日志输出时使用utf-8编码集显示 git config --global i18n.logoutputencoding utf-8 # 注释：设置LESS字符集为utf-8 export LESSCHARSET=utf-8 Git LFS 介绍 Git 大文件存储（Large File Storage，简称LFS）目的是更好地把大型二进制文件，比如音频文件、数据集、图像和视频等集成到 Git 的工作流中。我们知道，Git 存储二进制效率不高，因为它会压缩并存储二进制文件的所有完整版本，随着版本的不断增长以及二进制文件越来越多，这种存储方案并不是最优方案。而 LFS 处理大型二进制文件的方式是用文本指针替换它们，这些文本指针实际上是包含二进制文件信息的文本文件。文本指针存储在 Git 中，而大文件本身通过HTTPS托管在Git LFS服务器上。 安装 git lfs install Updated git hooks. Git LFS initialized. LFS追踪文件 git lfs track \"*.zip\" git lfs track \"Anaconda3-4.4.0-Linux-x86_64.sh\" 执行命令后，将会在项目中生成.gitattributes文件，该文件保存文件的追踪记录，需要将该文件推送到远程仓库当中。 查看追踪规则 git lfs track Listing tracked patterns *.iso (.gitattributes) grafana-6.4.3-1.x86_64.rpm (.gitattributes) Listing excluded patterns 提交&推送 与git基本操作一致，使用”git add“，”git commit“，”git push“命令。 # 添加 git add Anaconda3-4.4.0-Linux-x86_64.sh # 提交 git commit -m \"添加大文件 Anaconda3-4.4.0-Linux-x86_64.sh\" # 推送 git push 查看追踪列表 git lfs ls-files 194faa7784 * grafana-6.4.3-1.x86_64.rpm 克隆&拉取 与git基本操作一致，使用git clone,git pull命令。 # 克隆 git clone git@gitlab.example.com:group/project.git # 拉取 git lfs fetch origin master 可执行权限 # 查看文件filename的权限 git ls-files --stage filename > 100644 7365f0cb8734bd2031ddf800e90 0 filename # 增加文件filename的可执行权限 git update-index --chmod +x filename # 再次查看文件filename的权限 git ls-files --stage filename > 100755 7365f0cb8734bd2031ddf800e90 0 filename # 忽略权限更改 # 当前仓库 git config core.filemode false # 全局 git config --global core.fileMode false 1. https://docs.gitlab.com/ee/administration/lfs/manage_large_binaries_with_git_lfs.html ↩ Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-05-21 20:27:48 "},"chapter4/Maven.html":{"url":"chapter4/Maven.html","title":"4.2 Maven","keywords":"","body":"4.2 Maven [TOC] 忽略测试用例 # 不执行测试用例，也不编译测试用例类 mvn package -Dmaven.test.skip=true # 不执行测试用例，但编译测试用例类生成相应的class文件至target/test-classes下 mvn package -DskipTests 刷新本地缓存 # mvn源更新后，刷新本地缓存 mvn clean package -U Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-14 10:08:04 "},"chapter4/IDEA.html":{"url":"chapter4/IDEA.html","title":"4.3 IDEA","keywords":"","body":"4.3 IDEA 开启控制台颜色 # Edit Configurations，点击，找到VM options添加 -Dspring.output.ansi.enabled=ALWAYS 添加启动命令 Edit Configurations->Env Vars: \"spring.profiles.active=dev\" 匹配注释 (/\\*([^*]|[\\r\\n]|(\\*+([^*/]|[\\r\\n])))*\\*+/|[ \\t]*//.*) # 或 (/\\*([^*]|[\\r\\n]|(\\*+([^*/]|[\\r\\n])))*\\*+/) Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-29 10:29:16 "},"chapter4/Nginx.html":{"url":"chapter4/Nginx.html","title":"4.4 Nginx","keywords":"","body":"4.4 Nginx [TOC] 安装 yum install -y nginx 配置文件夹 /etc/nginx 配置示例 # For more information on configuration, see: # * Official English Documentation: http://nginx.org/en/docs/ # * Official Russian Documentation: http://nginx.org/ru/docs/ user www; worker_processes auto; error_log /var/log/nginx/error.log; pid /run/nginx.pid; # Load dynamic modules. See /usr/share/doc/nginx/README.dynamic. include /usr/share/nginx/modules/*.conf; events { worker_connections 1024; } http { log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; # Load modular configuration files from the /etc/nginx/conf.d directory. # See http://nginx.org/en/docs/ngx_core_module.html#include # for more information. include /etc/nginx/conf.d/*.conf; server { listen 80 default_server; listen [::]:80 default_server; server_name www.dalongm.top @.dalongm.top; rewrite ^(.*)$ https://$host$1 permanent; } server { listen 443 default_server ssl; listen [::]:443 default_server ssl; server_name www.dalongm.top @.dalongm.top; ssl_certificate cert/3961127_dalongm.top.pem; #将domain name.pem替换成您证书的文件名。 ssl_certificate_key cert/3961127_dalongm.top.key; #将domain name.key替换成您证书的密钥文件名。 ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; #使用此加密套件。 ssl_protocols TLSv1.2; #使用该协议进行配置。 ssl_prefer_server_ciphers on; location / { proxy_pass https://127.0.0.1:9443; proxy_set_header Host $host; } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } server { listen 80; listen [::]:80; server_name score.dalongm.top; rewrite ^(.*)$ https://$host$1 permanent; } server { listen 443 ssl; listen [::]:443 ssl; server_name score.dalongm.top; ssl_certificate cert/3961362_score.dalongm.top.pem; #将domain name.pem替换成您证书的文件名。 ssl_certificate_key cert/3961362_score.dalongm.top.key; #将domain name.key替换成您证书的密钥文件名。 ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; #使用此加密套件。 ssl_protocols TLSv1.2; #使用该协议进行配置。 ssl_prefer_server_ciphers on; location / { proxy_pass https://127.0.0.1:9200; proxy_set_header Host $host; } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } } Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-05-27 13:04:35 "},"chapter5/":{"url":"chapter5/","title":"第五章 大数据","keywords":"","body":"第五章 大数据组件 5.1 Kafka 5.2 ES 5.3 HDFS 5.4 Hive 5.5 Flume 5.6 Zookeeper 5.7 Flink Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-24 19:37:30 "},"chapter5/Kafka.html":{"url":"chapter5/Kafka.html","title":"5.1 Kafka","keywords":"","body":"5.1 Kafka [TOC] 安装 #!/bin/bash BASE_DIR=$(readlink -f $(dirname $0)) cd ${BASE_DIR} INSTALL_PATH=/usr/local tar -zxf kafka_2.11-2.4.1.tgz rm -rf ${INSTALL_PATH}/kafka_2.11-2.4.1 rm -rf ${INSTALL_PATH}/kafka mv kafka_2.11-2.4.1 ${INSTALL_PATH} ln -s ${INSTALL_PATH}/kafka_2.11-2.4.1 ${INSTALL_PATH}/kafka 系统服务 配置文件kafka.service [Unit] Description=Apache Kafka: A Distributed Streaming Platform After=network.target [Service] Type=simple User=root Group=root Restart=always RestartSec=1 ExecStart=/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties ExecStop=/usr/local/kafka/bin/kafka-server-stop.sh [Install] WantedBy=multi-user.target 添加自启与启动 #!/bin/bash BASE_DIR=$(readlink -f $(dirname $0)) cd ${BASE_DIR} INSTALL_PATH=/usr/local ${INSTALL_PATH}/kafka/bin/kafka-server-stop.sh rm -f /usr/lib/systemd/system/kafka.service cp kafka.service /usr/lib/systemd/system systemctl daemon-reload systemctl enable kafka.service systemctl stop kafka.service systemctl start kafka.service 启动Zookeeper /usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties & 启动Kafka Broker /usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties & 配置文件 /usr/local/kafka/config/server.properties # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # see kafka.server.KafkaConfig for additional details and defaults ############################# Server Basics ############################# # The id of the broker. This must be set to a unique integer for each broker. broker.id=0 ############################# Socket Server Settings ############################# # The address the socket server listens on. It will get the value returned from # java.net.InetAddress.getCanonicalHostName() if not configured. # FORMAT: # listeners = listener_name://host_name:port # EXAMPLE: # listeners = PLAINTEXT://your.host.name:9092 listeners=PLAINTEXT://localhost:9092 # Hostname and port the broker will advertise to producers and consumers. If not set, # it uses the value for \"listeners\" if configured. Otherwise, it will use the value # returned from java.net.InetAddress.getCanonicalHostName(). advertised.listeners=PLAINTEXT://localhost:9092 # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL # The number of threads that the server uses for receiving requests from the network and sending responses to the network num.network.threads=3 # The number of threads that the server uses for processing requests, which may include disk I/O num.io.threads=8 # The send buffer (SO_SNDBUF) used by the socket server socket.send.buffer.bytes=102400 # The receive buffer (SO_RCVBUF) used by the socket server socket.receive.buffer.bytes=102400 # The maximum size of a request that the socket server will accept (protection against OOM) socket.request.max.bytes=104857600 ############################# Log Basics ############################# # A comma separated list of directories under which to store log files log.dirs=/tmp/kafka-logs # The default number of log partitions per topic. More partitions allow greater # parallelism for consumption, but this will also result in more files across # the brokers. num.partitions=1 # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown. # This value is recommended to be increased for installations with data dirs located in RAID array. num.recovery.threads.per.data.dir=1 ############################# Internal Topic Settings ############################# # The replication factor for the group metadata internal topics \"__consumer_offsets\" and \"__transaction_state\" # For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3. offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 ############################# Log Flush Policy ############################# # Messages are immediately written to the filesystem but by default we only fsync() to sync # the OS cache lazily. The following configurations control the flush of data to disk. # There are a few important trade-offs here: # 1. Durability: Unflushed data may be lost if you are not using replication. # 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush. # 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks. # The settings below allow one to configure the flush policy to flush data after a period of time or # every N messages (or both). This can be done globally and overridden on a per-topic basis. # The number of messages to accept before forcing a flush of data to disk #log.flush.interval.messages=10000 # The maximum amount of time a message can sit in a log before we force a flush #log.flush.interval.ms=1000 ############################# Log Retention Policy ############################# # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # The minimum age of a log file to be eligible for deletion due to age log.retention.hours=168 # A size-based retention policy for logs. Segments are pruned from the log unless the remaining # segments drop below log.retention.bytes. Functions independently of log.retention.hours. #log.retention.bytes=1073741824 # The maximum size of a log segment file. When this size is reached a new log segment will be created. log.segment.bytes=1073741824 # The interval at which log segments are checked to see if they can be deleted according # to the retention policies log.retention.check.interval.ms=300000 ############################# Zookeeper ############################# # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\". # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. zookeeper.connect=localhost:2181 # Timeout in ms for connecting to zookeeper zookeeper.connection.timeout.ms=6000 ############################# Group Coordinator Settings ############################# # The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance. # The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms. # The default value for this is 3 seconds. # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing. # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup. group.initial.rebalance.delay.ms=0 限制日志文件大小 /usr/local/kafka/config/log4j.properties # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # Unspecified loggers and loggers with additivity=true output to server.log and stdout # Note that INFO only applies to unspecified loggers, the log level of the child logger is used otherwise log4j.rootLogger=INFO, stdout, kafkaAppender log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.kafkaAppender=org.apache.log4j.RollingFileAppender # 单个日志文件大小 log4j.appender.kafkaAppender.MaxFileSize=128MB # 保留历史文件个数 log4j.appender.kafkaAppender.MaxBackupIndex=10 log4j.appender.kafkaAppender.append=true # log4j.appender.kafkaAppender.DatePattern='.'yyyy-MM-dd-HH log4j.appender.kafkaAppender.File=${kafka.logs.dir}/server.log log4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout log4j.appender.kafkaAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.stateChangeAppender=org.apache.log4j.RollingFileAppender # 单个日志文件大小 log4j.appender.stateChangeAppender.MaxFileSize=128MB # 保留历史文件个数 log4j.appender.stateChangeAppender.MaxBackupIndex=10 log4j.appender.stateChangeAppender.append=true # log4j.appender.stateChangeAppender.DatePattern='.'yyyy-MM-dd-HH log4j.appender.stateChangeAppender.File=${kafka.logs.dir}/state-change.log log4j.appender.stateChangeAppender.layout=org.apache.log4j.PatternLayout log4j.appender.stateChangeAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.requestAppender=org.apache.log4j.RollingFileAppender # 单个日志文件大小 log4j.appender.requestAppender.MaxFileSize=128MB # 保留历史文件个数 log4j.appender.requestAppender.MaxBackupIndex=10 log4j.appender.requestAppender.append=true # log4j.appender.requestAppender.DatePattern='.'yyyy-MM-dd-HH log4j.appender.requestAppender.File=${kafka.logs.dir}/kafka-request.log log4j.appender.requestAppender.layout=org.apache.log4j.PatternLayout log4j.appender.requestAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.cleanerAppender=org.apache.log4j.RollingFileAppender # 单个日志文件大小 log4j.appender.cleanerAppender.MaxFileSize=128MB # 保留历史文件个数 log4j.appender.cleanerAppender.MaxBackupIndex=10 log4j.appender.cleanerAppender.append=true # log4j.appender.cleanerAppender.DatePattern='.'yyyy-MM-dd-HH log4j.appender.cleanerAppender.File=${kafka.logs.dir}/log-cleaner.log log4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayout log4j.appender.cleanerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.controllerAppender=org.apache.log4j.RollingFileAppender # 单个日志文件大小 log4j.appender.controllerAppender.MaxFileSize=128MB # 保留历史文件个数 log4j.appender.controllerAppender.MaxBackupIndex=10 log4j.appender.controllerAppender.append=true # log4j.appender.controllerAppender.DatePattern='.'yyyy-MM-dd-HH log4j.appender.controllerAppender.File=${kafka.logs.dir}/controller.log log4j.appender.controllerAppender.layout=org.apache.log4j.PatternLayout log4j.appender.controllerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.authorizerAppender=org.apache.log4j.RollingFileAppender # 单个日志文件大小 log4j.appender.authorizerAppender.MaxFileSize=128MB # 保留历史文件个数 log4j.appender.authorizerAppender.MaxBackupIndex=10 log4j.appender.authorizerAppender.append=true # log4j.appender.authorizerAppender.DatePattern='.'yyyy-MM-dd-HH log4j.appender.authorizerAppender.File=${kafka.logs.dir}/kafka-authorizer.log log4j.appender.authorizerAppender.layout=org.apache.log4j.PatternLayout log4j.appender.authorizerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n # Change the two lines below to adjust ZK client logging log4j.logger.org.I0Itec.zkclient.ZkClient=INFO log4j.logger.org.apache.zookeeper=INFO # Change the two lines below to adjust the general broker logging level (output to server.log and stdout) log4j.logger.kafka=INFO log4j.logger.org.apache.kafka=INFO # Change to DEBUG or TRACE to enable request logging log4j.logger.kafka.request.logger=WARN, requestAppender log4j.additivity.kafka.request.logger=false # Uncomment the lines below and change log4j.logger.kafka.network.RequestChannel$ to TRACE for additional output # related to the handling of requests #log4j.logger.kafka.network.Processor=TRACE, requestAppender #log4j.logger.kafka.server.KafkaApis=TRACE, requestAppender #log4j.additivity.kafka.server.KafkaApis=false log4j.logger.kafka.network.RequestChannel$=WARN, requestAppender log4j.additivity.kafka.network.RequestChannel$=false log4j.logger.kafka.controller=TRACE, controllerAppender log4j.additivity.kafka.controller=false log4j.logger.kafka.log.LogCleaner=INFO, cleanerAppender log4j.additivity.kafka.log.LogCleaner=false log4j.logger.state.change.logger=TRACE, stateChangeAppender log4j.additivity.state.change.logger=false # Access denials are logged at INFO level, change to DEBUG to also log allowed accesses log4j.logger.kafka.authorizer.logger=INFO, authorizerAppender log4j.additivity.kafka.authorizer.logger=false 基本操作 查看kafka topics /usr/local/kafka/bin/kafka-topics.sh --list --zookeeper localhost:2181 新建topic /usr/local/kafka/bin/kafka-topics.sh --create --replication-factor 1 --partitions 1 --zookeeper localhost:2181 --topic ${topic} 删除kafka topics /usr/local/kafka/bin/kafka-topics.sh --delete --zookeeper localhost:2181 --topic ${topic} 产生消息 /usr/local/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic ${topic} 接收消息 /usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic ${topic} 查看topic信息 /usr/local/kafka/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic ${topic} 重置组消费位置 bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --to-earliest --execute bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --topic ${topic} --to-earliest --execute 无法启动 ERROR Shutdown broker because all log dirs in /tmp/kafka-logs have failed 删除/tmp/kafka-logs下的offset和consume相关文件，重启。此方法有可能造成数据丢失。 修改advertised.listeners=PLAINTEXT://:9092，重启 修改log.dirs=/tmp/kafka-logs，重启 Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-06-01 10:10:04 "},"chapter5/ES.html":{"url":"chapter5/ES.html","title":"5.2 ES","keywords":"","body":"5.2 ES [TOC] 查看mapping curl -XGET '127.0.0.1:9200/noah_app_access_20200101/_mapping/?pretty=true' 统计某个字段的数目 http://host:9200/_sql?sql=SELECT count(distinct(resource_id)) as num FROM index_* 查看索引数据 POST http://host:9200/index_*/_search?pretty Body { \"size\":1000 } 删除索引 curl -XDELETE '127.0.0.1:9200/index_*' Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-14 10:08:04 "},"chapter5/HDFS.html":{"url":"chapter5/HDFS.html","title":"5.3 HDFS","keywords":"","body":"5.3 HDFS [TOC] 退出安全模式 bin/hadoop dfsadmin -safemode leave 进入安全模式 bin/hadoop dfsadmin -safemode enter 恢复edits不一致 bin/hadoop namenode -recover 在HDFS中，提供了fsck命令，用于检查HDFS上文件和目录的健康状态、获取文件的block信息和位置信息等。 fsck命令必须由HDFS超级用户来执行，普通用户无权限。 查看文件中损坏的块（-list-corruptfileblocks） [root@master sbin]$ hdfs fsck / -list-corruptfileblocks 将损坏的文件移动至/lost+found目录（-move） [root@master sbin]$ hdfs fsck / -move 删除损坏的文件（-delete） [root@master sbin]$ hdfs fsck / -delete 检查并列出所有文件状态（-files） [root@master sbin]$ hdfs fsck / -files 查看dfs块的报告 [root@master sbin]$ hdfs dfsadmin -report 查看目录 [root@master sbin]$ hdfs -ls / HDFS datanode无法启动 Directory /data05/block is in an inconsistent state: cluster Id is incompatible with others. 停止namenode 删除data的current内容 rm -rf /data*/block/current/* 格式化 ./hdfs namenode -format 重启 Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-05-15 20:07:41 "},"chapter5/Hive.html":{"url":"chapter5/Hive.html","title":"5.4 Hive","keywords":"","body":"5.4 Hive Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-13 16:34:39 "},"chapter5/Flume.html":{"url":"chapter5/Flume.html","title":"5.5 Flume","keywords":"","body":"5.5 Flume Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-13 16:34:46 "},"chapter5/Zookeeper.html":{"url":"chapter5/Zookeeper.html","title":"5.6 Zookeeper","keywords":"","body":"5.6 Zookeeper [TOC] 安装 #!/bin/bash BASE_DIR=$(readlink -f $(dirname $0)) cd ${BASE_DIR} INSTALL_PATH=/usr/local tar -zxf apache-zookeeper-3.6.0-bin.tar.gz rm -rf ${INSTALL_PATH}/apache-zookeeper-3.6.0-bin rm -rf ${INSTALL_PATH}/zookeeper mv apache-zookeeper-3.6.0-bin ${INSTALL_PATH}/ ln -s ${INSTALL_PATH}/apache-zookeeper-3.6.0-bin ${INSTALL_PATH}/zookeeper cp ${INSTALL_PATH}/zookeeper/conf/zoo_sample.cfg ${INSTALL_PATH}/zookeeper/conf/zoo.cfg 系统服务 配置文件zookeeper.service [Unit] Description=Apache ZooKeeper highly reliable distributed coordination After=network.target [Service] Type=simple User=root Group=root Restart=always RestartSec=1 ExecStart=/usr/local/zookeeper/bin/zkServer.sh start-foreground ExecStop=/usr/local/zookeeper/bin/zkServer.sh stop [Install] WantedBy=multi-user.target 添加自启与启动 #!/bin/bash BASE_DIR=$(readlink -f $(dirname $0)) cd ${BASE_DIR} INSTALL_PATH=/usr/local ${INSTALL_PATH}/zookeeper/bin/zkServer.sh stop rm -f /usr/lib/systemd/system/zookeeper.service cp zookeeper.service /usr/lib/systemd/system systemctl daemon-reload systemctl enable zookeeper.service systemctl stop zookeeper.service systemctl start zookeeper.service Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-06-01 10:10:04 "},"chapter5/Flink.html":{"url":"chapter5/Flink.html","title":"5.7 Flink","keywords":"","body":"5.7 Flink [TOC] 安装 #!/bin/bash BASE_DIR=$(readlink -f $(dirname $0)) cd ${BASE_DIR} INSTALL_PATH=/usr/local tar -zxf flink-1.9.2-bin-scala_2.11.tgz DIR=flink-1.9.2 rm -rf ${INSTALL_PATH}/flink-1.9.2 rm -rf ${INSTALL_PATH}/flink mv ${DIR} ${INSTALL_PATH} chown -R root ${INSTALL_PATH}/${DIR} chgrp -R root ${INSTALL_PATH}/${DIR} ln -s ${INSTALL_PATH}/${DIR} ${INSTALL_PATH}/flink 注册系统服务(standalone模式) 配置文件 /usr/lib/systemd/system/flink.service [Unit] Description=Apache Flink: Stateful Computations over Data Streams After=network.target [Service] Type=forking User=root Group=root Restart=always RestartSec=1 ExecStart=/usr/local/flink/bin/start-cluster.sh ExecStop=/usr/local/flink/bin/stop-cluster.sh [Install] WantedBy=multi-user.target 开机自启 #!/bin/bash BASE_DIR=$(readlink -f $(dirname $0)) cd ${BASE_DIR} INSTALL_PATH=/usr/local ${INSTALL_PATH}/kafka/bin/kafka-server-stop.sh rm -f /usr/lib/systemd/system/kafka.service cp kafka.service /usr/lib/systemd/system # 重载配置 systemctl daemon-reload # 启用自启 systemctl enable kafka.service # 停止 systemctl stop kafka.service # 启动 systemctl start kafka.service 启动（使用系统服务代替） ${INSTALL_PATH}/flink/bin/start-cluster.sh 停止（使用系统服务代替） ${INSTALL_PATH}/flink/bin/stop-cluster.sh 配置文件 ${INSTALL_PATH}/flink/conf/log4j.properties ################################################################################ # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # \"License\"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################ #============================================================================== # Common #============================================================================== # The external address of the host on which the JobManager runs and can be # reached by the TaskManagers and any clients which want to connect. This setting # is only used in Standalone mode and may be overwritten on the JobManager side # by specifying the --host parameter of the bin/jobmanager.sh executable. # In high availability mode, if you use the bin/start-cluster.sh script and setup # the conf/masters file, this will be taken care of automatically. Yarn/Mesos # automatically configure the host name based on the hostname of the node where the # JobManager runs. jobmanager.rpc.address: localhost # The RPC port where the JobManager is reachable. jobmanager.rpc.port: 6123 # The heap size for the JobManager JVM jobmanager.heap.size: 1024m # The heap size for the TaskManager JVM taskmanager.heap.size: 1024m # The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline. taskmanager.numberOfTaskSlots: 1 # The parallelism used for programs that did not specify and other parallelism. parallelism.default: 1 # The default file system scheme and authority. # # By default file paths without scheme are interpreted relative to the local # root file system 'file:///'. Use this to override the default and interpret # relative paths relative to a different file system, # for example 'hdfs://mynamenode:12345' # # fs.default-scheme #============================================================================== # High Availability #============================================================================== # The high-availability mode. Possible options are 'NONE' or 'zookeeper'. # # high-availability: zookeeper # The path where metadata for master recovery is persisted. While ZooKeeper stores # the small ground truth for checkpoint and leader election, this location stores # the larger objects, like persisted dataflow graphs. # # Must be a durable file system that is accessible from all nodes # (like HDFS, S3, Ceph, nfs, ...) # # high-availability.storageDir: hdfs:///flink/ha/ # The list of ZooKeeper quorum peers that coordinate the high-availability # setup. This must be a list of the form: # \"host1:clientPort,host2:clientPort,...\" (default clientPort: 2181) # # high-availability.zookeeper.quorum: localhost:2181 # ACL options are based on https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#sc_BuiltinACLSchemes # It can be either \"creator\" (ZOO_CREATE_ALL_ACL) or \"open\" (ZOO_OPEN_ACL_UNSAFE) # The default value is \"open\" and it can be changed to \"creator\" if ZK security is enabled # # high-availability.zookeeper.client.acl: open #============================================================================== # Fault tolerance and checkpointing #============================================================================== # The backend that will be used to store operator state checkpoints if # checkpointing is enabled. # # Supported backends are 'jobmanager', 'filesystem', 'rocksdb', or the # . # # state.backend: filesystem # Directory for checkpoints filesystem, when using any of the default bundled # state backends. # # state.checkpoints.dir: hdfs://namenode-host:port/flink-checkpoints # Default target directory for savepoints, optional. # # state.savepoints.dir: hdfs://namenode-host:port/flink-checkpoints # Flag to enable/disable incremental checkpoints for backends that # support incremental checkpoints (like the RocksDB state backend). # # state.backend.incremental: false # The failover strategy, i.e., how the job computation recovers from task failures. # Only restart tasks that may have been affected by the task failure, which typically includes # downstream tasks and potentially upstream tasks if their produced data is no longer available for consumption. jobmanager.execution.failover-strategy: region #============================================================================== # Rest & web frontend #============================================================================== # The port to which the REST client connects to. If rest.bind-port has # not been specified, then the server will bind to this port as well. # #rest.port: 8081 # The address to which the REST client will connect to # #rest.address: 0.0.0.0 # Port range for the REST and web server to bind to. # #rest.bind-port: 8080-8090 # The address that the REST & web server binds to # #rest.bind-address: 0.0.0.0 # Flag to specify whether job submission is enabled from the web-based # runtime monitor. Uncomment to disable. #web.submit.enable: false #============================================================================== # Advanced #============================================================================== # Override the directories for temporary files. If not specified, the # system-specific Java temporary directory (java.io.tmpdir property) is taken. # # For framework setups on Yarn or Mesos, Flink will automatically pick up the # containers' temp directories without any need for configuration. # # Add a delimited list for multiple directories, using the system directory # delimiter (colon ':' on unix) or a comma, e.g.: # /data1/tmp:/data2/tmp:/data3/tmp # # Note: Each directory entry is read from and written to by a different I/O # thread. You can include the same directory multiple times in order to create # multiple I/O threads against that directory. This is for example relevant for # high-throughput RAIDs. # # io.tmp.dirs: /tmp # Specify whether TaskManager's managed memory should be allocated when starting # up (true) or when memory is requested. # # We recommend to set this value to 'true' only in setups for pure batch # processing (DataSet API). Streaming setups currently do not use the TaskManager's # managed memory: The 'rocksdb' state backend uses RocksDB's own memory management, # while the 'memory' and 'filesystem' backends explicitly keep data as objects # to save on serialization cost. # # taskmanager.memory.preallocate: false # The classloading resolve order. Possible values are 'child-first' (Flink's default) # and 'parent-first' (Java's default). # # Child first classloading allows users to use different dependency/library # versions in their application than those in the classpath. Switching back # to 'parent-first' may help with debugging dependency issues. # # classloader.resolve-order: child-first # The amount of memory going to the network stack. These numbers usually need # no tuning. Adjusting them may be necessary in case of an \"Insufficient number # of network buffers\" error. The default min is 64MB, the default max is 1GB. # # taskmanager.network.memory.fraction: 0.1 # taskmanager.network.memory.min: 64mb # taskmanager.network.memory.max: 1gb #============================================================================== # Flink Cluster Security Configuration #============================================================================== # Kerberos authentication for various components - Hadoop, ZooKeeper, and connectors - # may be enabled in four steps: # 1. configure the local krb5.conf file # 2. provide Kerberos credentials (either a keytab or a ticket cache w/ kinit) # 3. make the credentials available to various JAAS login contexts # 4. configure the connector to use JAAS/SASL # The below configure how Kerberos credentials are provided. A keytab will be used instead of # a ticket cache if the keytab path and principal are set. # security.kerberos.login.use-ticket-cache: true # security.kerberos.login.keytab: /path/to/kerberos/keytab # security.kerberos.login.principal: flink-user # The configuration below defines which JAAS login contexts # security.kerberos.login.contexts: Client,KafkaClient #============================================================================== # ZK Security Configuration #============================================================================== # Below configurations are applicable if ZK ensemble is configured for security # Override below configuration to provide custom ZK service name if configured # zookeeper.sasl.service-name: zookeeper # The configuration below must match one of the values set in \"security.kerberos.login.contexts\" # zookeeper.sasl.login-context-name: Client #============================================================================== # HistoryServer #============================================================================== # The HistoryServer is started and stopped via bin/historyserver.sh (start|stop) # Directory to upload completed jobs to. Add this directory to the list of # monitored directories of the HistoryServer as well (see below). #jobmanager.archive.fs.dir: hdfs:///completed-jobs/ # The address under which the web-based HistoryServer listens. #historyserver.web.address: 0.0.0.0 # The port under which the web-based HistoryServer listens. #historyserver.web.port: 8082 # Comma separated list of directories to monitor for completed jobs. #historyserver.archive.fs.dir: hdfs:///completed-jobs/ # Interval in milliseconds for refreshing the monitored directories. #historyserver.archive.fs.refresh-interval: 10000 修改日志大小 ${INSTALL_PATH}/flink/conf/log4j.properties ################################################################################ # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # \"License\"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################ # This affects logging for both user code and Flink log4j.rootLogger=INFO, file # Uncomment this if you want to _only_ change Flink's logging #log4j.logger.org.apache.flink=INFO # The following lines keep the log level of common libraries/connectors on # log level INFO. The root logger does not override this. You have to manually # change the log levels here. log4j.logger.akka=INFO log4j.logger.org.apache.kafka=INFO log4j.logger.org.apache.hadoop=INFO log4j.logger.org.apache.zookeeper=INFO # Log all infos in the given file log4j.appender.file=org.apache.log4j.RollingFileAppender log4j.appender.file.file=${log.file} # 单个日志文件大小 log4j.appender.file.MaxFileSize=128MB # 保留历史文件个数 log4j.appender.file.MaxBackupIndex=10 log4j.appender.file.append=true log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n # Suppress the irrelevant (wrong) warnings from the Netty channel handler log4j.logger.org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, file ${INSTALL_PATH}/flink/conf/log4j-cli.properties ################################################################################ # Licensed to the Apache Software Foundation (ASF) under one # or more contributor license agreements. See the NOTICE file # distributed with this work for additional information # regarding copyright ownership. The ASF licenses this file # to you under the Apache License, Version 2.0 (the # \"License\"); you may not use this file except in compliance # with the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################################################################################ log4j.rootLogger=INFO, file # Log all infos in the given file log4j.appender.file=org.apache.log4j.RollingFileAppender log4j.appender.file.file=${log.file} # 单个日志文件大小 log4j.appender.file.MaxFileSize=128MB # 保留历史文件个数 log4j.appender.file.MaxBackupIndex=10 log4j.appender.file.append=true log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n # Log output from org.apache.flink.yarn to the console. This is used by the # CliFrontend class when using a per-job YARN cluster. log4j.logger.org.apache.flink.yarn=INFO, console log4j.logger.org.apache.flink.yarn.cli.FlinkYarnSessionCli=INFO, console log4j.logger.org.apache.hadoop=INFO, console log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n # suppress the warning that hadoop native libraries are not loaded (irrelevant for the client) log4j.logger.org.apache.hadoop.util.NativeCodeLoader=OFF # suppress the irrelevant (wrong) warnings from the netty channel handler log4j.logger.org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, file 访问Flink Web http://localhost:8081 提交任务 ./flink [OPTIONS] [ARGUMENTS] The following actions are available: Action \"run\" compiles and runs a program. Syntax: run [OPTIONS] \"run\" action options: -c,--class Class with the program entry point (\"main()\" method or \"getPlan()\" method). Only needed if the JAR file does not specify the class in its manifest. -C,--classpath Adds a URL to each user code classloader on all nodes in the cluster. The paths must specify a protocol (e.g. file://) and be accessible on all nodes (e.g. by means of a NFS share). You can use this option multiple times for specifying more than one URL. The protocol must be supported by the {@link java.net.URLClassLoader}. -d,--detached If present, runs the job in detached mode -n,--allowNonRestoredState Allow to skip savepoint state that cannot be restored. You need to allow this if you removed an operator from your program that was part of the program when the savepoint was triggered. -p,--parallelism The parallelism with which to run the program. Optional flag to override the default value specified in the configuration. -py,--python 指定Python作业的入口，依赖的资源文件可以通过 `--pyFiles`进行指定。 -pyfs,--pyFiles 指定Python作业依赖的一些自定义的python文件， 如果有多个文件，可以通过逗号(,)进行分隔。支持 常用的python资源文件，例如(.py/.egg/.zip)。 (例如:--pyFiles file:///tmp/myresource.zip ,hdfs:///$namenode_address/myresource2.zip) -pym,--pyModule 指定python程序的运行的模块入口，这个选项必须配合 `--pyFiles`一起使用。 -q,--sysoutLogging If present, suppress logging output to standard out. -s,--fromSavepoint Path to a savepoint to restore the job from (for example hdfs:///flink/savepoint-1537). -sae,--shutdownOnAttachedExit If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C. Options for yarn-cluster mode: -d,--detached If present, runs the job in detached mode -m,--jobmanager Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -sae,--shutdownOnAttachedExit If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C. -yat,--yarnapplicationType Set a custom application type for the application on YARN -yD use value for given property -yd,--yarndetached If present, runs the job in detached mode (deprecated; use non-YARN specific option instead) -yh,--yarnhelp Help for the Yarn session CLI. -yid,--yarnapplicationId Attach to running YARN session -yj,--yarnjar Path to Flink jar file -yjm,--yarnjobManagerMemory Memory for JobManager Container with optional unit (default: MB) -yn,--yarncontainer Number of YARN container to allocate (=Number of Task Managers) -ynm,--yarnname Set a custom name for the application on YARN -yq,--yarnquery Display available YARN resources (memory, cores) -yqu,--yarnqueue Specify YARN queue. -ys,--yarnslots Number of slots per TaskManager -yst,--yarnstreaming Start Flink in streaming mode -yt,--yarnship Ship files in the specified directory (t for transfer), multiple options are supported. -ytm,--yarntaskManagerMemory Memory per TaskManager Container with optional unit (default: MB) -yz,--yarnzookeeperNamespace Namespace to create the Zookeeper sub-paths for high availability mode -ynl,--yarnnodeLabel Specify YARN node label for the YARN application -z,--zookeeperNamespace Namespace to create the Zookeeper sub-paths for high availability mode Options for default mode: -m,--jobmanager Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -z,--zookeeperNamespace Namespace to create the Zookeeper sub-paths for high availability mode Action \"info\" shows the optimized execution plan of the program (JSON). Syntax: info [OPTIONS] \"info\" action options: -c,--class Class with the program entry point (\"main()\" method or \"getPlan()\" method). Only needed if the JAR file does not specify the class in its manifest. -p,--parallelism The parallelism with which to run the program. Optional flag to override the default value specified in the configuration. Action \"list\" lists running and scheduled programs. Syntax: list [OPTIONS] \"list\" action options: -r,--running Show only running programs and their JobIDs -s,--scheduled Show only scheduled programs and their JobIDs Options for yarn-cluster mode: -m,--jobmanager Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -yid,--yarnapplicationId Attach to running YARN session -z,--zookeeperNamespace Namespace to create the Zookeeper sub-paths for high availability mode Options for default mode: -m,--jobmanager Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -z,--zookeeperNamespace Namespace to create the Zookeeper sub-paths for high availability mode Action \"stop\" stops a running program with a savepoint (streaming jobs only). Syntax: stop [OPTIONS] \"stop\" action options: -d,--drain Send MAX_WATERMARK before taking the savepoint and stopping the pipelne. -p,--savepointPath Path to the savepoint (for example hdfs:///flink/savepoint-1537). If no directory is specified, the configured default will be used (\"state.savepoints.dir\"). Options for yarn-cluster mode: -m,--jobmanager Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -yid,--yarnapplicationId Attach to running YARN session -z,--zookeeperNamespace Namespace to create the Zookeeper sub-paths for high availability mode Options for default mode: -m,--jobmanager Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -z,--zookeeperNamespace Namespace to create the Zookeeper sub-paths for high availability mode Action \"cancel\" cancels a running program. Syntax: cancel [OPTIONS] \"cancel\" action options: -s,--withSavepoint **DEPRECATION WARNING**: Cancelling a job with savepoint is deprecated. Use \"stop\" instead. Trigger savepoint and cancel job. The target directory is optional. If no directory is specified, the configured default directory (state.savepoints.dir) is used. Options for yarn-cluster mode: -m,--jobmanager Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -yid,--yarnapplicationId Attach to running YARN session -z,--zookeeperNamespace Namespace to create the Zookeeper sub-paths for high availability mode Options for default mode: -m,--jobmanager Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -z,--zookeeperNamespace Namespace to create the Zookeeper sub-paths for high availability mode Action \"savepoint\" triggers savepoints for a running job or disposes existing ones. Syntax: savepoint [OPTIONS] [] \"savepoint\" action options: -d,--dispose Path of savepoint to dispose. -j,--jarfile Flink program JAR file. Options for yarn-cluster mode: -m,--jobmanager Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -yid,--yarnapplicationId Attach to running YARN session -z,--zookeeperNamespace Namespace to create the Zookeeper sub-paths for high availability mode Options for default mode: -m,--jobmanager Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -z,--zookeeperNamespace Namespace to create the Zookeeper sub-paths for high availability mode 关于App资源分配 使用yarn-cluster模式可以通过yjm和ytm参数指定yarnjobManagerMemory、yarntaskManagerMemory内存大小，单机集群模式不能在提交任务时指定（因为在启动flink时，已经在flink-conf.yaml里分配了这两内存）。 Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-06-01 10:10:04 "},"chapter6/":{"url":"chapter6/","title":"第六章 教程","keywords":"","body":"第六章 教程 6.1 Flink 中文课程 Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-21 20:23:45 "},"chapter6/Flink 中文课程.html":{"url":"chapter6/Flink 中文课程.html","title":"6.1 Flink 中文课程","keywords":"","body":"6.1 Flink 中文课程 [TOC] https://github.com/flink-china/flink-training-course 课程简介：本系列课程由 Apache Flink Community China 官方出品。旨在为具备一定大数据基础、对Apache Flink感兴趣的同学提供系统性的入门教程，课程路径为“基础篇（已完结）>>进阶篇（已完结）>>运维篇&实战篇（进行中）>>源码篇”，下拉本页面查看已完结课程。 「彩蛋」 大数据领域顶级盛会 Flink Forward Asia 2019 精彩回看🔎 https://url.cn/5tGC874 Apache Flink 中文邮件列表使用方式 视频 Flink中文学习网站地址：https://ververica.cn/developers/flink-training-course3/ S3 实战&运维篇（进行中） 备注：S1 S2 基础篇+进阶篇本页下滑可见 3.1 Flink 反压/延时监控和调参控制 PPT 视频回放 讲师：Rong Rong（Apache Flink Committer，Software Engineer at Uber） 3.2 Metric 指标、监控、报警 PPT 视频回放 讲师：孙梦瑶（美团点评研发工程师） 3.3 如何利用 Flink 和深度学习模型实现垃圾图片分类（Apache Flink 极客挑战赛特别场） 视频回放 讲师：陈戊超（阿里巴巴技术专家) 3.4 Flink CEP 实战 PPT 视频回放 讲师：刘博（哈啰出行大数据实时平台资深开发） 3.5 Flink 实时数仓的应用 PPT 视频回放 讲师：黄伟伦（美团实时数仓技术专家） 3.6 State Processor API 介绍与演示 PPT 视频回放 讲师：戴资力（Apache Flink PMC） 3.7 Flink 常见问题诊断 PPT 视频回放 讲师：杨阳（阿里巴巴 高级运维工程师） 3.8 大规模场景的高阶运维 PPT 视频回放 讲师：王华（阿里巴巴 运维专家） 3.9 Flink 作业问题分析和调优实践 PPT 视频回放 讲师：李康（虎牙 实时平台负责人） 3.10 Flink 生产配置最佳实践 PPT 视频回放 讲师：席建刚 趣头条实时平台负责人 3.11 基于 Apache Flink 的监控告警系统 PPT 视频回放 讲师：zhisheng（《Flink 实战与性能优化》专栏作者） 3.12 30分钟教你如何入门 – Apache Flink 最全知识图谱详解 知识图谱 PPT 视频回放 讲师：程鹤群（军长）（Apache Flink Committer，阿里巴巴技术专家） 3.13 Demo: 基于 Flink SQL 构建离线应用 PPT 视频回放 讲师：李劲松（之信）（Apache Beam Committer，阿里巴巴技术专家） 3.14 Demo: 基于 Flink SQL 构建实时应用 PPT 视频回放 讲师：伍翀（云邪）（Apache Flink PMC，阿里巴巴技术专家） 3.15 【1.10特别篇】Flink on Zeppelin: 极致体验(1) 入门 + Batch PPT 视频回放 讲师：章剑锋（Apache Zeppelin PMC，阿里巴巴高级技术专家） 3.16 【实时数仓篇】利用 Flink 实现典型的实时 ETL 场景 PPT 视频回放 讲师：买蓉（美团点评） 3.17 【1.10特别篇】Flink on Zeppelin: 极致体验(2) Streaming + 高级用法 PPT 视频回放 讲师：章剑锋（Apache Zeppelin PMC，阿里巴巴高级技术专家） 3.18 【1.10特别篇】Alink 入门到实践 PPT 视频回放 讲师：杨旭（阿里巴巴资深算法专家） 3.19 【实时数仓篇】利用 Flink 实现实时超时统计场景 PPT 视频回放 讲师：马汶园（菜鸟 数据工程师） 3.20 【实时数仓篇】利用 Flink 实现实时状态复用场景 PPT 视频回放 讲师：李晨（菜鸟 数据工程师） 3.21 【1.10特别篇】PyFlink 架构、应用案例及未来规划 PPT 视频回放 讲师：孙金城（Apache Flink PMC，Apache Beam Committer， 阿里巴巴高级技术专家） 3.22 【1.10特别篇】Running Flink on Kubernetes natively PPT 视频回放 讲师：王阳（阿里巴巴实时计算引擎团队研发专家） 3.23 【1.10特别篇】Flink TaskExecutor 内存管理与配置 PPT 视频回放 讲师：宋辛童（Apache Flink Contributor，阿里巴巴高级开发工程师） 3.24 【1.10特别篇】container 环境实战 PPT 视频回放 讲师：唐云（Apache Flink Contributor，阿里巴巴高级开发工程师) 3.26 Flink 完美搭档 – Pravega：架构总览 PPT 视频回放 讲师：滕昱（DellEMC 技术总监) 3.26 【实时数仓篇】Flink 窗口函数的应用场景 PPT 视频回放 讲师：张俊（Apache Flink Contributor，OPPO大数据平台研发负责人） 3.27 PyFlink 核心功能介绍 讲师：程鹤群（军长）（Apache Flink Committer，阿里巴巴技术专家） 直播：4月16日 20:00-21:00 （UTC+8） 备注：视频、PPT待直播后更新 3.28【实时数仓篇】深入解读 Flink 资源管理机制 讲师：宋辛童（Apache Flink Contributor，阿里巴巴高级开发工程师） 直播：4月21日 20:00-21:00 （UTC+8） 备注：视频、PPT待直播后更新 S2 进阶篇（已完结） 备注：S1基础篇本页下滑可见 2.1 Flink Runtime 核心机制剖析 PPT 视频回放 第一课文章 讲师：高赟（Apache Flink Contributor，阿里巴巴高级开发工程师) 2.2 Flink Time 深度解析 PPT 视频回放 第二课文章 讲师：崔星灿（Apache Flink Committer，加拿大约克大学博士后) 2.3 Flink Checkpoint-轻量级分布式快照 PPT 视频回放 第三课文章 讲师：唐云（Apache Flink Contributor，阿里巴巴高级开发工程师) 2.4 Flink on Yarn/K8S原理剖析及实践 PPT 视频回放 第四课文章 讲师：周凯波（Apache Flink Contributor，阿里巴巴技术专家） 2.5 Flink 数据类型与序列化 PPT 视频回放 讲师：马庆祥（Apache Flink Contributor，360数据开发高级工程师） 2.6 Flink 作业执行解析 PPT 视频回放 讲师：岳猛（Apache Flink Contributor，网易云音乐实时计算平台研发工程师） 2.7 Flink 网络流控及反压剖析 PPT 视频回放 讲师：张俊（Apache Flink Contributor，OPPO大数据平台研发负责人） 2.8 Metrics 与监控 PPT 视频回放 讲师：刘彪（Apache Flink Contributor，阿里巴巴技术专家) 2.9 Flink Connector开发 PPT 视频回放 讲师：董亭亭 快手实时计算引擎团队负责人 2.10 本地部署Zeppelin开发Flink程序 PPT 视频回放 讲师：章剑锋（Apache Zeppelin PMC，阿里巴巴高级技术专家） 2.11 Flink State 最佳实践 PPT 视频回放 讲师：唐云（Apache Flink Contributor，阿里巴巴高级开发工程师) Intel特邀课程： Take advantage of Intel Optane DCPM in Flink workload PPT 视频回放 讲师：马艳 （Intel Software Engineer） 2.12 TensorFlow On Flink PPT 视频回放 讲师：陈戊超（阿里巴巴技术专家) Intel特邀课程：Anlytics-Zoo 构建统一的大数据分析+AI流水线 PPT 视频回放 讲师：史栋杰（Intel 资深软件架构师） 2.13 深度探索 Flink SQL PPT 视频回放 讲师：贺小令（Apache Flink Contributor，阿里巴巴技术专家) 2.14 Apache Flink Python API 现状及规划 PPT 视频回放 讲师：孙金城（Apache Flink PMC，阿里巴巴高级技术专家) S1 基础篇 （已完结） 1.1 为什么要学习 Apache Flink？ PPT点我 视频回放点我 1&2课文章 讲师：陈守元（阿里巴巴高级产品专家） 1.2 Flink基本概念 PPT点我 视频回放点我 1&2课文章 讲师：戴资力（Apache Flink PMC) 1.3 Flink 安装部署、环境配置及运行应用程序 PPT点我 视频回放点我 第三课文章 讲师：沙晟阳（阿里巴巴高级开发工程师) 1.4 DataStream API编程 PPT点我 视频回放点我 第四课文章 讲师：崔星灿（Apache Flink Committer) 1.5 客户端操作 PPT点我 视频回放点我 第五课文章 讲师：周凯波（阿里巴巴技术专家） 1.6 Window & Time PPT点我 视频回放点我 第六课文章 讲师：邱从贤（阿里巴巴高级开发工程师） 1.7 状态管理与容错机制 PPT点我 视频回放点我 第七课文章 讲师：孙梦瑶（美团点评研发工程师) 1.8 Flink Table API 编程 PPT点我 视频回放点我 第八课文章 讲师：程鹤群（Apache Flink Contributor) 1.9 Flink SQL 编程 PPT点我 视频回放点我 第九课文章 讲师：伍翀（Apache Flink Committer) Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-24 19:40:36 "},"END.html":{"url":"END.html","title":"结语","keywords":"","body":"结语 Copyright © dalongm.top 2020 all right reserved，powered by Gitbook修改时间： 2020-04-07 12:03:36 "}}